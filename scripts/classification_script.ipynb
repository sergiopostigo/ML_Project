{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Agricultural Exports Categories Analysis**\n",
    "*by Sergio Postigo and Víctor Diví*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **1. Introduction**\n",
    "Many countries base a big portion of their economies in foreign trade. Therefore, the customs agencies around the world collect data about every imported/exported good that passed across their ports, airports, borders, etc. In some countries, this data is open, and anyone can access to it to analyze it and make more informed decisions while importing or exporting goods. However, this data demands some challenges before its use. One of them is the labelling. For example, in Peru every time a good is imported/exported, a customs agent fills a form with the information of the product(s), where they include descriptions about it. Nonetheless, there isn´t a proper labelling, for instance in case we wanted to aggregate amounts imported/exported by category.\n",
    "\n",
    "The labelling for goods is done manually mainly by consultancy agencies, who get this data to generate analytics reports for companies and institutions interested in foreign trade information of specific products. They usually use MS Excel spreadsheets to perform the labeling, which is not efficient and takes much time. We want to address this pain by automatizing the labelling process using Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **2. Data Wrangling**\n",
    "The data was provided from a consultancy company in Peru called RTM. They were hired by an agricultural exports company who were interested in knowing which categories of products were exported from Peru from 2017 till 2021. RTM adquired the data from the company Veritrade, who consolidate foreign trade databases from many conuntries in South America. \n",
    "\n",
    "RTM provided us the data in .xlsx format. We converted it into CSV (using Excel) and then proceed to import it into this notebook as DataFrame with the Pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import Pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the CSV data into Dataframe\n",
    "data =  pd.read_csv(\"../data/raw_data/data.csv\", encoding='latin-1', sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we don't have a separated training data source, we will split the data available into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We use 20% of the data to test and 80% to train\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **3. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first describe the columns of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Partida Aduanera: Specific code of a product included in the Harmonized System of the World Customs Organization (WCO)\n",
    "\n",
    "2. Descripcion de la Partida Aduanera: Description about the product associated with the customs code\n",
    "\n",
    "3. Aduana: Customs office from which the export was performed\n",
    "\n",
    "4. DUA:  Single Administrative Document, it is a document that gathers information about the shipping\n",
    "\n",
    "5. Fecha: Shipping date\n",
    "\n",
    "6. Año: Shipping year\n",
    "\n",
    "7. Cod. Tributario: Tax code of the company exporting the good\n",
    "\n",
    "8. Exportador en Perú: Company or entity exporting the good\n",
    "\n",
    "9. Importador Extranjero: Company or entity importing the good\n",
    "\n",
    "10. Kg Bruto: Weight of the good in kg, including the weight of the container or box\n",
    "\n",
    "11. Kg Neto: Weight of good in kg, excluding the weight of the container or box\n",
    "\n",
    "12. Toneladas Netas: Weight of good in tons, excluding the weight of the container or box\n",
    "\n",
    "13. Qty 1: Quantity of the good in terms of a specific measurement unit (1)\n",
    "\n",
    "14. Und 1: Unit of measurement (1)\n",
    "\n",
    "15. Qty 2: Quantity of the good in terms of a specific measurement unit (2)\n",
    "\n",
    "16. Und 2:  Unit of measurement (2)\n",
    "\n",
    "17. U$ FOB Tot: The value of the goods at the exporter's customs frontier in USD\n",
    "\n",
    "18. Miles de USD Fob TOTAL: The value of the goods at the exporter's customs frontier in thousands of USD\n",
    "\n",
    "19. U$ FOB Und 1: The value of the goods by unity (1)\n",
    "\n",
    "20. U$ FOB Und 2: The value of the goods by unity (2)\n",
    "\n",
    "21. Pais de Destino: Country of destiny\n",
    "\n",
    "22. Puerto de destino: Port of destiny\n",
    "\n",
    "23. Último Puerto Embarque: Last port of shipment\n",
    "\n",
    "24. Via: Via (air, see, maritime)\n",
    "\n",
    "25. Agente Portuario: Port agent\n",
    "\n",
    "26. Agente de Aduana: Customs agent\n",
    "\n",
    "27. Descripcion Comercial: Commercial description of the good\n",
    "\n",
    "28. Descripcion1: Commercial description portion 1\n",
    "\n",
    "29. Descripcion2: Commercial description portion 2\n",
    "\n",
    "30. Descripcion3: Commercial description portion 3\n",
    "\n",
    "31. Descripcion4: Commercial description portion 4\n",
    "\n",
    "32. Descripcion5: Commercial description portion 5\n",
    "\n",
    "33. Naviera: Shipping company\n",
    "\n",
    "34. Agente Carga(Origen): Load Agent (origin)\n",
    "\n",
    "35. Agente Carga(Destino): Load Agent (destiny)\n",
    "\n",
    "36. Canal: Selectivity channe. Type of control that the Customs Service will carry out on the merchandise to be exported. There are three channels: Green, Orange and Red\n",
    "\n",
    "37. Concatenar: Column that concatenates 27, 28, 29, 30, 31, 32\n",
    "\n",
    "38. Categoría macro Aurum: Designated category/label\n",
    "\n",
    "39. Subcategoría inicial: Designated subcategories/sub-lables\n",
    "\n",
    "40. Subcategoría Consolidada Aurum:  Designated subcategories/sub-lables (with less granularity, it groups some sub-categories in \"others\")\n",
    "\n",
    "41. Categoría Consolidada Aurum: Designated category/label (with less granularity, it groups some categories in \"others\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Remark 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All posible categories all labeled in *Categoria macro Aurum* and all possible subcategories are labelled in *Subcategoria inicial*. Aurum grouped some of the categories in *Categoria consolidada Aurum* as \"others\" and did the same in *Subcategoría Consolidada Aurum* for the subcategories. This last two columns were very likely a requirement frem their client. He may have been interested specially in a list of categories and the rest were simply labeled as \"others\". However, what is from interest from us are the colums with all the categories and all the subcategories (*Categoria macro Aurum* and *Subcategoria inicial*). <br>\n",
    "\n",
    "**Since the categories can be mapped from the subcategories, the model(s) to work on should predict the subcategories that are in the column *Subcategria inicial***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's analyze the distribution of the target class: *Subcategoría inicial*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(train['Subcategoría inicial'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check count of appearances of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train['Subcategoría inicial'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As seen, there is an important class imbalance. Let's show this in an histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take the first 75 rows\n",
    "train['Categoría macro Aurum'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By analyzing the first 75 rows we can see that there is already an important class unbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Remark 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " *Descripcion1*, *Descripcion2*, *Descripcion3*, *Descripcion4* and *Descripcion5* concatenaded build *Descripcion Comercial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replace NaN values with ''\n",
    "train.fillna('', inplace=True)\n",
    "\n",
    "# Concatenate Descripcion1, Descripcion2, Descripcion3, Descripcion4 and Descripcion5 and save the string in column \"train\"\n",
    "train['Concatenated_Descriptions'] = train['Descripcion1'] + ' ' + train['Descripcion2'] + ' ' + train['Descripcion3'] + ' ' + train['Descripcion4'] + ' ' + train['Descripcion5']\n",
    "\n",
    "# Remove spaces before and after the string\n",
    "train['Concatenated_Descriptions'] = train['Concatenated_Descriptions'].str.strip()\n",
    "\n",
    "# Compare \"Descripcion Comercial\" and \"train\"\n",
    "train['Equal?'] = train['Descripcion Comercial'] == train['Concatenated_Descriptions']\n",
    "\n",
    "# Print the comparison\n",
    "temp_df = train[['Descripcion Comercial', 'Concatenated_Descriptions', 'Equal?']]\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check if all are equal\n",
    "print(\"From \"+str(temp_df[[\"Equal?\"]].describe().values[0][0])+\" rows, *Descripcion Comercial* and the *Concatenated_Descriptions* are equal in \"+str(temp_df[[\"Equal?\"]].describe().values[3][0])+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp_df[temp_df['Equal?']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check what happens in a row where *Equal?* is false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Descrpicion Comercial\n",
    "print(temp_df.iloc[215653]['Descripcion Comercial'])\n",
    "# Concatenated_Descriptions\n",
    "print(temp_df.iloc[215653]['Concatenated_Descriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems that when trimming the column *Descripcion Comercial* into *Description* 1,2,3,4 and 5, Veritrade removed some characters, in this case some white spaces. That is why when we reconstruct the *Concatenated_Descriptions* from *Description* 1,2,3,4 and 5 we don´t get the exactly same string as in *Descripcion Comercial*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Additionally, we can make an additional remark here: *Descripcion Comercial* has repeated sentences in it´s values, as is showed in the example above. This must be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the columns used to explain this remark\n",
    "train.drop(columns=['Concatenated_Descriptions', 'Equal?'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Remark 3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The column *Concatenar* concatenates *Descripcion Comercial* and *Descripcion1*, *2,3,4* and *5*. Thus, it basically has a concatenation of two times the string from *Descripcion Comercial*. It seems that the consultants didn´t know that *Descripcion1*, *2,3,4* and *5* are trims of *Descripcion Comercial*. Maybe they thought this extra columns contained additional information and that is why they decided to concatenate everything in the \"Concatenar\" column to then process the information from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Remark 4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For each *Partida aduanera* there is only one possible *Descripcion de la partida aduanera*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporal dataframe \n",
    "temp_df = train[[\"Partida Aduanera\", \"Descripcion de la Partida Aduanera\"]].copy()\n",
    "# Remove duplicated rows\n",
    "temp_df.drop_duplicates(inplace=True)\n",
    "# Get the number of rows\n",
    "print(\"The number of all combinations of the columns *Partida Aduanera* and *Descripcion de la Partida Aduanera* are \"+str(temp_df.shape[0]))\n",
    "# Get the number of unique values of *Partida Aduanera*\n",
    "print(\"The number of unique values of the column *Partida Aduanera* is \"+str(temp_df['Partida Aduanera'].nunique()))\n",
    "# Get the number of unique values of *Descripcion de la Partida Aduanera*\n",
    "print(\"The number of unique values of the column *Descripcion de la Partida Aduanera* is \"+str(temp_df['Descripcion de la Partida Aduanera'].nunique()))\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are some values of *Descripcion de la partida aduanera* that correspond to multiple values of *Partida Aduanera*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the values of *Descripcion de la Partida Aduanera* that are related to multiple values of *Partida Aduanera*\n",
    "temp_df[temp_df.duplicated(['Descripcion de la Partida Aduanera'], keep=False)].sort_values(by=['Descripcion de la Partida Aduanera'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is very likely an error in the customs agency systems, we will have to deal with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Remark 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we are dealing with agricultural items, we can presume that there is an seasonal influence in the dates in which they are exported. Let's test this assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will compare the *Partida aduanera* with the date column names *Fecha*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the columns *Partida Aduanera* and *Fecha*\n",
    "temp_df = train[[\"Partida Aduanera\", \"Fecha\"]].copy()\n",
    "# Cast the column of date (*Fecha*) to datetime\n",
    "temp_df['Fecha'] = pd.to_datetime(temp_df['Fecha'], format='%d/%m/%Y')\n",
    "# Count the exports by date\n",
    "temp_df['Count of exports'] = temp_df.groupby(['Partida Aduanera','Fecha'])['Fecha'].transform('count')\n",
    "temp_df.drop_duplicates(inplace=True)\n",
    "# Sort the dataframe\n",
    "temp_df.sort_values(by=['Partida Aduanera', 'Fecha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sample randomly some values of \"Partida Aduanera\" to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import random library\n",
    "import random\n",
    "\n",
    "# Number of samples\n",
    "samples_qty = 6\n",
    "# Get samples from *Partida Aduanera* (without repetitions)\n",
    "samples = random.sample(list(dict.fromkeys(temp_df['Partida Aduanera'].tolist())), samples_qty)\n",
    "print(\"The random selected values from *Partida Aduanera* are: \")\n",
    "print(samples)\n",
    "# Create a list with the dataframes of each sample\n",
    "samples_dfs = []\n",
    "for sample in samples:\n",
    "    samples_dfs.append(temp_df[temp_df['Partida Aduanera'] == sample ].sort_values(by=[\"Partida Aduanera\", \"Fecha\"])[['Fecha', 'Count of exports']])\n",
    "# For each dataframe, populate the missing dates (imputing values of 0 for Count of exports)\n",
    "populated_samples_dfs =[]\n",
    "for sample_df in samples_dfs:\n",
    "    dates = pd.date_range(sample_df['Fecha'].min(),sample_df['Fecha'].max())\n",
    "    sample_df.set_index('Fecha', inplace=True)\n",
    "    sample_df = sample_df.reindex(dates, fill_value=0) #this cant be done inplace\n",
    "    populated_samples_dfs.append(sample_df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's plot all the selected samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for i in range(0,len(populated_samples_dfs)-1):\n",
    "    #----------------------------------------------------------------\n",
    "    # GRAPH\n",
    "    #----------------------------------------------------------------\n",
    "\n",
    "    # size:\n",
    "    plt.figure(figsize=(30,3))\n",
    "\n",
    "    # title:\n",
    "    plt.title('Shipments of '+ str(samples[i])+' by day', fontsize=20)\n",
    "\n",
    "    # x axis:\n",
    "    # x values\n",
    "    x = range(0,len(populated_samples_dfs[i].index.date.tolist()))\n",
    "    # x ticks\n",
    "    my_xticks = populated_samples_dfs[i].index.date.tolist()\n",
    "    for c in range(0, len(my_xticks)):\n",
    "        my_xticks[c] = my_xticks[c].strftime('%h-%Y')\n",
    "    plt.xticks(x[::30], my_xticks[::30], rotation='vertical')\n",
    "    # # x label\n",
    "    plt.xlabel(\"date\", fontsize=16)\n",
    "\n",
    "    # y axis:\n",
    "    # y values\n",
    "    y = populated_samples_dfs[i][\"Count of exports\"].tolist()\n",
    "    # y ticks\n",
    "    #plt.yticks(np.arange(populated_samples_dfs[i][\"Count of exports\"].min(), populated_samples_dfs[i][\"Count of exports\"].max()+1, 1))\n",
    "\n",
    "    # y label\n",
    "    plt.ylabel(\"# shipments\", fontsize=16)\n",
    "\n",
    "    # create plot\n",
    "    plt.plot(x, y, marker='o')\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    # show plot\n",
    "    plt.show()\n",
    "\n",
    "    #----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, some products (represented by its *Partida Aduanera* number) present a seasonal pattern (as expected) but others not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Remark 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The custom agents in Peru fill the columns of *Importador Extranjero*. Thus, we presume that there may be inconsistencies in the naming of the same company in different rows. Let's check this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We take as an example the *Importador Extranjero* value of \"Comercial Agricola Montoliva Ltda.\". Let's check the rows have a similar name (more than 90% similarity using Levenshtein Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import Levenshtein\n",
    "import Levenshtein\n",
    "\n",
    "# Get the colum *Importador Extranjero*\n",
    "df=pd.DataFrame(train, columns=['Importador Extranjero'])\n",
    "# Add a column with the similarity magnitude according to Levenshtein Algorithm\n",
    "df[\"Similarity\"]=df.apply(lambda x: Levenshtein.ratio(x['Importador Extranjero'],  \"Comercial Agricola Montoliva Ltda.\"), axis=1)\n",
    "# Filter rows with more than 90% similarity\n",
    "df.iloc[(df[\"Similarity\"]>=0.90).values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As seen, rows refering to the same company in the *Importador Extranjero* column, have slightly different values. This must be cleaned in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check now for *Exportador en Peru*. We will use a random company as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the colum *Importador Extranjero*\n",
    "df=pd.DataFrame(train, columns=['Exportador en Perú'])\n",
    "# Random company\n",
    "company = df.sample()['Exportador en Perú'].values[0]\n",
    "print(\"The company to be analyzed is: \"+company)\n",
    "# Add a column with the similarity magnitude according to Levenshtein Algorithm\n",
    "df[\"Similarity\"]=df.apply(lambda x: Levenshtein.ratio(x['Exportador en Perú'],  company), axis=1)\n",
    "# Filter rows with more than 90% similarity\n",
    "df.iloc[(df[\"Similarity\"]>=0.9).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check how many unique values are in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[(df[\"Similarity\"]>=0.8).values].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We know from Aurum that the column *Exportador en Perú* is filled from a dropdown menu. So, since it's not \"typed\" there are no inconsistencies. This is alligned with the results we are getting above regarding this column. As such, there is no need to clean this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Remark 7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The column *Descripcion de la Partida Aduanera* gives general information about the asociated product code of *Partida aduanera*, while the column \"Descripcion Comercial\" contains more detailed information. Let's study this colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select the three columns of interest\n",
    "df = train[[\"Partida Aduanera\", \"Descripcion de la Partida Aduanera\", \"Descripcion Comercial\"]]\n",
    "# Get the cardinality of *Partida aduanera*\n",
    "print(\"There are \"+ str(len(df['Partida Aduanera'].unique()))+ \" different codes of Partida Aduanera in total\")\n",
    "# Get the cardinality of *Descripcion de la Partida Aduanera*\n",
    "print(\"There are \"+ str(len(df['Descripcion de la Partida Aduanera'].unique()))+ \" different values of Descripcion de la Partida Aduanera in total\")\n",
    "# Get the cardinality of *Partida aduanera*\n",
    "print(\"There are \"+ str(len(df['Descripcion Comercial'].unique()))+ \" different values of Descripcion Comercial in total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **4. Data cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this stage we will clean the data and specifically the columns that we will use in the model(s) in the next section. Of course, we don´t need to clean all the columns, since many of them are not relevant for labeling the rows. So, let's first determine the columns to be used and justify why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "| COLUMN | USEFUL | JUSTIFICATION |\n",
    "| --- | --- | --- |\n",
    "| Partida Aduanera | NO | For each customs code there is one description in *Descripcion de la Partida Aduanera*. This last carries more information about the product. So, we won´t take this attribute and consider the next one. |\n",
    "| Descripcion de la Partida Aduanera | **YES** | This is a general description about the product, so this carries valuable information for the labeling |\n",
    "| Aduana | NO | The port from which the product is beeing shipped. For now, we won´t consider it for our models |\n",
    "| DUA | NO | This is a random generated code associated with the shipping, it does not carry information that can be captured |\n",
    "| Fecha | **YES** | Associating the date of shipping to a category is insightfull. As we saw, some products are exported in specific seasons of the year |\n",
    "| Año | NO | Already included in the previous attribute |\n",
    "| Cod. Tributario | NO| There is one tax code for each company. A company can be associated to specific groups of products, however the amount of different companies can be huge.  |\n",
    "| Exportador en Perú | NO | Same idea as previous row |\n",
    "| Importador Extranjero | NO | The amount of different importers abroad may be huge and new data my carry new names not learned by the model |\n",
    "| Kg Bruto | NO | See next attribute |\n",
    "| Kg Neto | **YES** | The weight of the shipments is insightfull, but is highly variable among same products, so initially we won´t use this feature. However we will use it to calculate the price by kg, which is actually insightfull |\n",
    "| Toneladas Netas | NO  | See previous attribute |\n",
    "| Qty 1 | NO | Same as before |\n",
    "| Und 1 | NO | Same as before |\n",
    "| Qty 2 | NO | Same as before |\n",
    "| Und 2 | NO | Same as before |\n",
    "| U$ FOB Tot | **YES** | The cost of the shipment will be use to calulate the cost by kg of the product |\n",
    "| Miles de USD Fob TOTAL | NO | It is just a repetition of the previous attribute |\n",
    "| U$ FOB Und 1 | NO | |\n",
    "| U$ FOB Und 2 | NO  |  |\n",
    "| Pais de Destino | **YES** | The country were this products are beeing imported can be related to groups of products |\n",
    "| Puerto de destino | NO | The previous attribute indirectly captures this information already |\n",
    "| Último Puerto Embarque | NO | |\n",
    "| Via | NO |  |\n",
    "| Agente Portuario | NO |  |\n",
    "| Agente de Aduana | NO  |  |\n",
    "| Descripcion Comercial | **YES** | The comercial description also carries valuable information for the labeling |\n",
    "| Descripcion1 | NO | Alredy captured in *Descripcion Comercial* |\n",
    "| Descripcion2 | NO | Alredy captured in *Descripcion Comercial* |\n",
    "| Descripcion3 | NO | Alredy captured in *Descripcion Comercial* |\n",
    "| Descripcion4 | NO | Alredy captured in *Descripcion Comercial* |\n",
    "| Descripcion5 | NO | Alredy captured in *Descripcion Comercial* |\n",
    "| Naviera | NO |  |\n",
    "| Agente Carga(Origen) | NO |  |\n",
    "| Agente Carga(Destino) | NO |  |\n",
    "| Canal | NO |  |\n",
    "| Concatenar | NO |  |\n",
    "| Categoría macro Aurum | NO | While we also need this category, it can be inferred given a prediction of the subcategory |\n",
    "| Subcategoría inicial | **YES** | **LABEL** |\n",
    "| Subcategoría Consolidada Aurum | NO |  |\n",
    "| Categoría Consolidada Aurum | NO |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train[[\"Descripcion de la Partida Aduanera\", \"Fecha\", \"Kg Neto\", \"U$ FOB Tot\", \"Pais de Destino\", \"Descripcion Comercial\", \"Categoría macro Aurum\" ]].copy()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From now on we will focus on each of the selected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Descripcion de la Partida Aduanera (description of the customs code)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data[[\"Descripcion de la Partida Aduanera\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since in this column we are dealing with textual descriptions of the product, we will use Natural Language Processing techniques. A first important step that we will perform is to remove the so called *stop words* from each cell, so that we get rid of the low-level information. For example, we see that the second row in the above table has the word 'Y' (and) or 'O' (or). This words should not be considered in our future model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To do this we will use the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import nltk\n",
    "# Download the stopwords feature\n",
    "nltk.download('stopwords')\n",
    "# Import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the stopword in Spanish\n",
    "sw_nltk = stopwords.words('spanish')\n",
    "print(\"The words considered stopwords in spanish are: \")\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's remove the stopwords, punctuations, accents and let's set the strings to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "# Create an array with the column values\n",
    "old_descriptions = train_data[\"Descripcion de la Partida Aduanera\"].tolist()\n",
    "\n",
    "# Array to store cleaned values\n",
    "new_descriptions = []\n",
    "\n",
    "# Remove the stopwords from each old cell and populate the new array\n",
    "for sentence in old_descriptions:\n",
    "    # Remove stopwords\n",
    "    words = [word for word in sentence.split() if word.lower() not in sw_nltk ]\n",
    "    new_text = \" \".join(words)\n",
    "    # Additionally remove punctuations\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    words = tokenizer.tokenize(new_text)\n",
    "    new_text = \" \".join(words)\n",
    "    # Set to lowercase\n",
    "    new_text = new_text.lower()\n",
    "    # Remove accents\n",
    "    new_text = unidecode.unidecode(new_text)\n",
    "    # Append to the array\n",
    "    new_descriptions.append(new_text)\n",
    "new_descriptions\n",
    "\n",
    "# Add the cleaned data to the training dataframe\n",
    "train_data[\"Descripcion de la Partida Aduanera\"] = new_descriptions\n",
    "train_data[[\"Descripcion de la Partida Aduanera\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Fecha (date)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For this column we will map the month of shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "date = train_data['Fecha'].tolist()\n",
    "date = pd.to_datetime(date, infer_datetime_format=True).month\n",
    "train_data['Fecha'] = date\n",
    "train_data['Fecha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Kg Neto (net weight in of good KG) and U$ FOB Tot (total price of good)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we said before, here we will get the price by kg of the good. To do this we will use both columns and transform them into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First, drop rows were weight is 0\n",
    "print(\"From \"+ str(len(train_data)) + \" rows there are \"+str(len(train_data[train_data['Kg Neto']==0]))+\" with weight = 0\")\n",
    "train_data.drop(train_data[train_data[\"Kg Neto\"] == 0].index, inplace=True)\n",
    "\n",
    "# Then divide the price over weight\n",
    "weight = train_data['Kg Neto'].str.replace(',','.').astype(float).values\n",
    "price = train_data['U$ FOB Tot'].str.replace(',','.').astype(float).values\n",
    "price_by_kg = np.divide(price, weight)\n",
    "\n",
    "# Drop the used columns\n",
    "train_data.drop(columns=[\"Kg Neto\", \"U$ FOB Tot\"], inplace=True)\n",
    "\n",
    "# Add the new column and name it usd_kg\n",
    "train_data[\"usd_kg\"]=price_by_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **País de destino (country of destiny)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "countries = train_data[\"Pais de Destino\"].unique()\n",
    "countries.sort()\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The column is correct and shows not corrupted data. We will only set the values to lowercase and remove accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "# Create an array with the column values\n",
    "old_countries = train_data[\"Pais de Destino\"].tolist()\n",
    "\n",
    "# Array to store cleaned values\n",
    "new_countries = []\n",
    "\n",
    "# Remove the stopwords from each old cell and populate the new array\n",
    "for country in old_countries:\n",
    "    # Set to lowercase\n",
    "    new_text = country.lower()\n",
    "    # Remove accents\n",
    "    new_text = unidecode.unidecode(new_text)\n",
    "    # Append to the array\n",
    "    new_countries.append(new_text)\n",
    "\n",
    "# Add the cleaned data to the training dataframe\n",
    "train_data[\"Pais de Destino\"] = new_countries\n",
    "train_data[[\"Pais de Destino\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Descripcion Comercial (comercial description)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As it will be shown below, there are values in this columns with repeated sentences inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comercial_description = train_data[\"Descripcion Comercial\"].tolist()\n",
    "comercial_description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's clean this and also remove accents, double or more white spaces, stopwords, punctuations and set to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to remove repeated sentences inside a same string\n",
    "def get_unrepeated_string(source: str) -> str:\n",
    "    return re.match(r'^\\s*([\\w\\s!\"#$%&\\'()*+,-./:;<=>?@{|}~º°«»\\[\\]§y¨`¦´¤¿]+?)(?:\\s*\\1)*\\s*$', source)[1]\n",
    "\n",
    "new_comercial_description = []\n",
    "\n",
    "for description in tqdm(comercial_description):\n",
    "    # First remove all accents\n",
    "    new_description = unidecode.unidecode(description)\n",
    "    # Remove two or more consecutive spaces and set one\n",
    "    new_description= ' '.join(new_description.split())\n",
    "    # Remove stopwords\n",
    "    words = [word for word in new_description.split() if word.lower() not in sw_nltk ]\n",
    "    new_description = \" \".join(words)\n",
    "    # Additionally remove punctuations\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    words = tokenizer.tokenize(new_description)\n",
    "    new_description = \" \".join(words)\n",
    "    # Set to lowercase\n",
    "    new_description = new_description.lower()\n",
    "    # Then remove the duplicated sentences\n",
    "    try:\n",
    "        new_comercial_description.append(get_unrepeated_string(new_description))\n",
    "    except:\n",
    "        print(new_description)\n",
    "\n",
    "train_data[\"Descripcion Comercial\"] = new_comercial_description\n",
    "new_comercial_description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **Subcategoría inicial (subcategories)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is the column to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, our data is clean and ready to be preprocessed. As a las step, we will reset the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data.to_csv('../data/cleaned_data/cleaned_data.csv', index=False)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **5. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this stage we will preprocess the data to be used in a classification model. As seen in the Data Exploration section, there is a big class inbalance. We will adress this issue as first step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the cleaned data\n",
    "train_data = pd.read_csv('../data/cleaned_data/cleaned_data.csv')\n",
    "# Select the target variable and the explanatory variables\n",
    "y = train_data[['Categoría macro Aurum']].values\n",
    "X = train_data.drop(['Categoría macro Aurum'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use a method called Oversampling, in which we will increase the low counts' classes by duplicating their rows as many times as needed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the library for Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Create the oversampling model\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "# Get the oversampled dataset\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since there was a big umbalance, the oversample generates too many extra rows. We will sample this new resampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We sample 100,000 rows from the new oversampled dataset\n",
    "idx = np.random.choice(np.arange(len(X_resampled)), 100000, replace=False)\n",
    "x_sample = X_resampled[idx]\n",
    "y_sample = y_resampled[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert the resample dataset into a dataframe and persist locally it for easy future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "resampled_train = np.column_stack((x_sample, y_sample))\n",
    "resampled_train = pd.DataFrame(resampled_train, columns=['Descripcion de la Partida Aduanera', 'Fecha', 'Pais de Destino', 'Descripcion Comercial', 'usd_kg', 'Subcategoría inicial'])\n",
    "resampled_train.to_csv(\"../data/preprocessed_data/resampled_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now recheck the class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resampled_train[\"Subcategoría inicial\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As shown above, now the classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are dealing with text, categorical and numerical data in this dataset. The next step will be then to represent the text columns as numbers, which is known as *sentence embedding*. This will be done in the columns *Descripcion de la Partida Aduanera* and *Descripcion Comercial*. Let's create a function to convert the text columns into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert a column with strings into vectors (the input is a list with the strings of the column)\n",
    "\n",
    "def col2vectors(rows):\n",
    "    # Import libraries for sentence embedding\n",
    "    from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "    import gensim\n",
    "    import gensim.downloader as api\n",
    "\n",
    "    # Get arrays of words for each row\n",
    "    data = [row.split() for row in rows]\n",
    "\n",
    "    # Create a TaggedDocument for each array (this is the input format for Doc2Vec)\n",
    "    def tagged_document(list_of_list_of_words):\n",
    "        for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "            yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "    # Get the data for training by converting the arrays to TaggedDocuments\n",
    "    data_for_training = list(tagged_document(data))\n",
    "\n",
    "    # Create the model\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=10, min_count=2, epochs=10)\n",
    "    model.build_vocab(data_for_training)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert *Descripcion de la Partida Aduanera (description of the customs code)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the column\n",
    "descriptions = X[\"Descripcion de la Partida Aduanera\"].values\n",
    "model = col2vectors(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the model locally for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"../models/custom_descriptions_doc2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert *Descripcion Comercial (comercial description)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's do the same for this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the column\n",
    "descriptions = X[\"Descripcion Comercial\"].values\n",
    "model = col2vectors(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the model locally for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"../models/comercial_descriptions_doc2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "descriptions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **6. Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section we will create predictive models using different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **6.1. Multi-Layer Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **6.1.1. Using only text colums**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### **6.1.1.1 Using *Descripcion Comercial* (comercial description)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the preprocessed data\n",
    "# training_data = pd.read_csv('../data/preprocessed_data/resampled_data.csv')\n",
    "training_data = X.copy()\n",
    "\n",
    "# Get the column and convert to vector using the doc2vec model trained before\n",
    "model = Doc2Vec.load('../models/comercial_descriptions_doc2vec_model')\n",
    "X = []\n",
    "for row in tqdm(training_data['Descripcion Comercial'].tolist()):\n",
    "    X.append(model.infer_vector(row.split(), epochs=10))\n",
    "\n",
    "# for i in tqdm(range(0,len(model.dv))):\n",
    "#     X.append(model.dv[i])\n",
    "\n",
    "# Get the target variable column\n",
    "y = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder().fit_transform(training_data['Pais de Destino'].values.reshape(-1,1))\n",
    "ohe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-3, hidden_layer_sizes=(16, 8, 8),\n",
    "                    random_state=1, max_iter=1000, early_stopping=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X[:], y[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(np.unique(y[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict([X[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted = clf.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original = y[:]\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(x == y for x, y in zip(original, predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "f,ax = plt.subplots(figsize=(20,20))\n",
    "cm = confusion_matrix(original, predicted)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot(ax=ax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('ml_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7741a1131fbe9bf5a0f5d7457e8f9d1e7d381e789d9a050b2713f6d313501fbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}